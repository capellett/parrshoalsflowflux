---
title: "A Comparison of Alternative Flow Fluctuation Metrics for Parr Shoals Reservoir Releases"
author: "CA Pellett"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{intro}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

Similar to many reservoirs in SC and globally, the streamflow downstream of the Parr Shoals dam often vary widely from hour to hour in an un-natural pattern caused by operations at the combined Parr Shoals-Monticello hydro and nuclear power projects. These artificial flow fluctuations can have detrimental impacts on the aquatic community, particularly during fish spawning periods. Recently, the dam operators have been demonstrating significant improvements in reducing these artificial flow fluctuations.  

I have attended meetings related to the FERC re-licensing of Parr Shoals and Monticello reservoirs for most of my career at SC DNR. I started in 2015, some time after the relicensing process had begun for this project, which followed multiple other relicensing projects for other reservoirs in SC. That is to say, there has always been a great deal of experience around the table, and I've learned a lot from participating. My own contributions have been, at most, minor, but the aspects related to the managed reservoir releases of water downstream have always held my interest; this is where my other experience of surface water modelling across the state seems most relevant.

In discussions with stakeholders on both sides of the table, it had seemed, to me, that significant improvements to the flow fluctuations might be unlikely, as there are a number of operational constraints and the costs of retro-fitting appeared astronomical. I have been delighted to see the innovative, cutting-edge, and apparently cost-effective solutions which have been implemented. The Parr Shoals operators have demonstrated significant and profound improvements - reducing the artificial flow fluctuations to provide more stable habitat downstream. With this increased capacity to modulate gate positions in response to varying reservoir levels, I propose that now is a timely opportunity to revisit and refine the metrics by which we evaluate flow-fluctuations. The metrics which have been used have been sufficient to demonstrate the improvements over time. However, as I've noted in previous comments, there are some limitations to those metrics which could confound the analysis. Now that a greater level of capability has been achieved, the AMP committee has an opportunity to fine-tune the metrics, potentially spurring further improvements without further costs. I want to be clear, this is not a critique of past efforts - my proposal of further refinement to the metrics is motivated by the demonstrable success of past efforts by the operators and their consultants.

The objectives of this memo are:
  1. Compile a flow dataset suitable for subsequent analysis.
  2. Develop a couple of alternative metrics to quantify the flow-fluctuations.
  3. Compare the metrics over time and during the spring spawning season.
  
This memo is a reproducible document generated using free and open-source software (R language and packages) and data from the USGS. It can be updated easily, or modified, for example to consider alternative metrics. 

## System Parameters
The following parameters are relevant to analyze the flow fluctuations.

### Spawning Periods
The exact period used for spawning has slipped my mind. For this analysis, I'll say the entire month of March, although I think it was only a couple of weeks.

```{r}
## an integer vector of length 2, 
## julian dates of start and end if spawning period
spawning_period <- c(
  lubridate::yday(lubridate::as_date('2020-03-01')),
  lubridate::yday(lubridate::as_date('2020-03-31')) )
## this gets messed up by leapyear.
## consider using a time-interval or something else from lubridate package.

## if you change the format of the spawning period object, 
## change the function below accordingly:
check_spawning_period <- function(date, spawning_period = spawning_period) {
  date_julian <- lubridate::yday(date)
  date_julian >= spawning_period[1] &
    date_julian <= spawning_period[2]}

```

### Hydro-electric Turbine Outflows
Below 4,800 cfs, all outflows are through the hydro-power turbines. That number will be lower if some turbines are not operational, and higher if the turbine capacity is increased. If the maximum turbine outflow has changed over the period of analysis, then this analysis should be refined to reflect those changes. I think one of the turbines was down for maintenance for a while, reducing the maximum turbine outflow to 3,200 cfs for a time period(?)

### Spill Release Outflows
When outflow is greater than 40,000 cfs, all gates are lowered to prevent damage to the dam.

### Detailed Gate Specifications 
Some more detailed specs on the gate operations could be relevant. The rotational speed of the gates, the number of gates, stage-flow relationships over the top of the gates(?). I don't have details at the moment, and perhaps they aren't necessary.

# Hydrology Data

The USGS maintains stream gages on the major incoming tributaries (Tyger, Enoree, and Broad rivers) to Lake Parr, and another stream gage on the outflow (Broad River at Alston). These datesets cover different time ranges, have different temporal resolutions (e.g. observations every 15 minutes or 1 hour), and may be missing some data. Here the gaged streamflow data is downloaded and combined in to a single dataset.

```{r data-download, eval=F, echo=F}
### This takes a few minutes to run. 
### Install the dataRetrieval package (developed by USGS), if needed.
# install.packages('dataRetrieval')

tyger0 <- dataRetrieval::readNWISuv(
  siteNumbers = '02160105', parameterCd = c('00060', '00065'))

enoree0 <- dataRetrieval::readNWISuv(
  siteNumbers = '02160700', parameterCd = c('00060', '00065'))

broad0 <- dataRetrieval::readNWISuv(
  siteNumbers = '02156500', parameterCd = c('00060', '00065'))

alston0 <- dataRetrieval::readNWISuv(
  siteNumbers = '02161000', parameterCd = c('00060', '00065'))

parr0 <- dataRetrieval::readNWISuv(
  siteNumbers = '02160990', parameterCd = '00065')


## if the data-raw folder doesn't exist, create it.
saveRDS(tyger0, '../data-raw/tyger0.rds')
saveRDS(enoree0, '../data-raw/enoree0.rds')
saveRDS(broad0, '../data-raw/broad0.rds')
saveRDS(alston0, '../data-raw/alston0.rds')
saveRDS(parr0, '../data-raw/parr0.rds')
```

### Gaged Inflows
```{r}
inflow_prep_function <- function(x) {
  x |>
  dplyr::arrange(dateTime) |>
  dplyr::mutate(dateTime_lag1 = dplyr::lag(dateTime),
                time_interval = lubridate::interval(
                  dateTime_lag1, dateTime) |>
                  lubridate::as.duration())
}

```

#### Tyger River Near Delta
USGS Gage No. 02160105 drains 759 sqmi.
```{r}
tyger0 <- readRDS('../data-raw/tyger0.rds') |>
  inflow_prep_function()
```

#### Enoree River at Whitmire
USGS Gage No. 02160700 drains 444 sqmi.
```{r}
enoree0 <- readRDS('../data-raw/enoree0.rds') |>
  inflow_prep_function()
```

#### Broad River at Carlisle
USGS Gage No. 02156500 drains 2,780 sqmi.
```{r}
broad0 <- readRDS('../data-raw/broad0.rds') |>
  inflow_prep_function()
```

### Gaged Outflow 
#### Broad River at Jenkinsville
USGS Gage No. 02160991 drains 4,750 sqmi. This gage was active before the reservoir was constructed. Could be relevant, but I'm not sure about it.

#### Broad River at Alston
USGS Gage No. 02160000 drains 4,790 sqmi. Located just a bit downriver of the Parr dam, this gage is representative of the outflows from the reservoir. I guess it drains about 40 more square miles (4790 - 4750?) than the impounded watershed.

```{r}
alston0 <- readRDS('../data-raw/alston0.rds') |>
  inflow_prep_function()
```


### Combined Flow Dataset
Total inflow can be estimated by combining the three inflow datasets. A simple sum has been used. Perhaps that can be refined. Lets review the data availability from the four stream gages first.

```{r}
flows0 <- dplyr::bind_rows(
  Broad=broad0, Enoree=enoree0, Tyger=tyger0, Alston=alston0, .id = 'gage')
```

#### Timestep Assessment
The different stream gage datasets may have different timesteps over time. Let's assess that graphically. First, I calculate the time interval between each observation and the prior observation at that gage site.

```{r}
## y = time_interval x = date
# time_interval_plot2 <- function(x) {x |>
flows0 |>
  ggplot2::ggplot(
    ggplot2::aes(x=dateTime, y=time_interval)) +
  ggplot2::geom_point(size=0.1) +
  ggplot2::scale_y_log10(
    breaks=c(60, 900, 3600, 86400, 604800, 2592000,
            31536000, 315360000, 3153600000),
    minor_breaks = NULL,
    labels=c("1m", '15m', "1h","1d","7d","30d",
            "1y", "10y", "100y")) +
  ggplot2::theme_bw() +
  ggplot2::facet_wrap('gage', ncol=1)
# }
```
The graph above shows that the gages generally have hourly data available starting after 1985 and 15-minute data starting around 2020. It looks like the Alston gage has 15 minute data going way back. That could be useful to show improvement over time, but I'm not going to look that far back at this point. There are some time intervals of over a week.

After combining the flow data in to a single table, with a row for each timestep and a column for each gage location, I'm going to remove rows which are missing data for any of the 4 gage locations (3 inflow gages). We might do better by trying to interpolate missing data, instead of simply removing it. But I think there is more than enough complete data in the sample, and interpolation could introduce error. 

```{r}
flows1 <- flows0 |>
  dplyr::select(gage, dateTime, flow = X_00060_00000) |>
  tidyr::pivot_wider(names_from = gage, values_from=flow) |>
  dplyr::filter(
    !is.na(Broad) & !is.na(Enoree) & 
      !is.na(Tyger) & !is.na(Alston)) |>
  inflow_prep_function()
```

```{r combined-time-interval-plot}
flows1 |>
  ggplot2::ggplot(
    ggplot2::aes(x=dateTime, y=time_interval)) +
  ggplot2::geom_point(size=0.1) +
  ggplot2::scale_y_log10(
    breaks=c(60, 900, 3600, 86400, 604800, 2592000,
            31536000, 315360000, 3153600000),
    minor_breaks = NULL,
    labels=c("1m", '15m', "1h","1d","7d","30d",
            "1y", "10y", "100y")) +
  ggplot2::theme_bw()

```
Here is a plot of the time-intervals between complete observations (observation events with data for all 4 gage sites). Apparently, there is very little missing data since the 15-minute observations began for all gages. Since 2020, there are no data gaps over 2 hours.

#### Missing data assessment
```{r combined-missing-data-plot}
flows2 <- flows1 |>
  dplyr::mutate(epoch = dplyr::if_else(
    dateTime < lubridate::as_datetime('2019-12-05 00:00:00'),
    '1hr', '15min'))

flow_test <- flows2 |>
  dplyr::mutate(
    denominator = dplyr::if_else(epoch == '1hr', 24, 96),
    day = lubridate::as_date(dateTime)) |>
  dplyr::group_by(epoch, day, denominator) |>
  dplyr::summarise(n = dplyr::n()) |>
  dplyr::ungroup() |>
  dplyr::mutate(missingObs = denominator - n)

range(flow_test$missingObs)

flow_test |>
  ggplot2::ggplot(
    ggplot2::aes(x=day, y=missingObs, color=epoch)) +
  ggplot2::geom_point(size=0.1) +
  ggplot2::theme_bw()
```
Since the 15-minute observations began, there have been no more than 10 missing observations in a single day. (The high point at the end of the graph is the day the data was downloaded, not a complete day of observations.)

```{r}
flow_test |>
  dplyr::filter(missingObs > 0) |>
  dplyr::mutate(year = lubridate::year(day)) |>
  dplyr::group_by(year) |>
  dplyr::summarise(`number of days with missing data`=dplyr::n()) |>
  dplyr::arrange(dplyr::desc(year))
```

## Data Sampling

For this analysis, I'll stick to the 15-minute data. I'll also label the observations according to flow flow at Alston (between 4,800 and 40,000 cfs, "Gate operation"; "Turbines only" if lower; "Spill" if higher).
```{r}
flows3 <- flows2 |>
  dplyr::filter(epoch == '15min') |>
  dplyr::mutate(outflow_type = dplyr::case_when(
    Alston < 4800 ~ 'Turbines only',
    Alston > 40000 ~ 'Spill',
    .default = 'Gate operation')) |>
  dplyr::select(-epoch, -dateTime_lag1)
```

```{r}
flows3 |>
  tidyr::pivot_longer(Broad:Alston) |>
  ggplot2::ggplot(
    ggplot2::aes(x=dateTime, y=value, group=name,
                 # alpha=outflow_type, 
                 color=name)) +
  ggplot2::geom_line() +
  ggplot2::theme_bw() +
  ggplot2::scale_y_log10(
    name = 'cfs', labels = scales::comma_format())
```
The Broad (at Carlisle) gage has nearly as much flow as the Alston gage (on a logarithmic scale, anyway), but this graph shows that the Alston gage has a lot more flow fluctuation in the gate operation range (4,800 to 40,000 cfs). I think I can see the difference, in 2020, when one of the turbines was not operational, and flow fluctuations affected flows lower than 4,800 cfs.

# Estimating "Natural" Outflows

The inflow data can be used to estimate "natural" outflows, that is, what the outflows might have been in the absence of the Parr Shoals-Monticello projects. Simply adding up the inflows is a good, simple, method, which might be refined by prorating the gaged inflows to account for ungauged area, and introducing a time lag or delay to account for the distance between the inflow gages and the outflow gage. Let's see if we can use the data to derive empirical estimates for area proration factors and lag times.

## Ungaged Inflows
There are some ungaged inflows both upstream and downstream of the dam. The total watershed area of the inflow gages is 759 + 444 + 2,780 = 3,983 square miles.

4750 - 3983 = 767 sqmi ungaged inflow area (upstream of the dam).
4790 - 4750 = 40 sqmi ungaged outflow area (downstream of the dam).

3983 / 4790 = 83% of watershed of the Alston gage is included in the inflow gage subwatersheds.

4790 / 3983 = 1.202611 proration factor. (Multiply the total gauged inflows by the proration factor to account for ungauged areas upstream of the Alston gage.) 

The proration factor, roughly 20%, is calculated as a ratio of watershed and subwatershed areas. Based on the drainage areas, we could assume that the flow at Alston will be about 20% greater than the sum of the inflow gages. We can also use an empirical method to calculate the proration factor.

```{r}
drainage_areas <- tibble::tribble(
  ~gage, ~sqmi,
  'Tyger', 759,
  'Enoree', 444,
  'Broad', 2780,
  'Inflow', 4790,
  'Alston', 4790)

flows4 <- flows3 |>
   dplyr::mutate(Inflow = Broad+Enoree+Tyger)
  
flows4 |>  
   dplyr::summarise(
     dplyr::across(c(Broad, Enoree, Tyger, Alston, Inflow),
                   mean)) |>
  dplyr::mutate(
    dplyr::across(
      c(Broad, Enoree, Tyger, Inflow),
    function(x) x/Alston)) |>
  dplyr::select(-Alston) |>
  tidyr::pivot_longer(cols=Broad:Inflow,
                      names_to = 'gage',
                      values_to = 'Percent of outflow') # |>
  # dplyr::left_join(drainage_areas, 'gage')
```

In a simpler world, the gaged Inflows would equal 83% of the outflows, instead they equal 87% (in this sample of observations). Whereas the watershed areas suggest that outflows would be about 20% more than the gaged inflows, they are empirically only about 15% more - this makes sense because the gaged inflow watersheds include areas of the upper piedmont and blue ridge regions which recieve more rainfall than the middle/lower piedmont, where the ungaged inflow area is.

## Lag Times

Let's start with an estimate of lag from the total of the inflows. Then let's try to estimate lag from each inflow gage separately.
```{r}
library(ggplot2)
library(ggpmisc)
```

I'll start by plotting the total gauged inflow against the outflows at Alston. 
```{r}
lag_calibration_plot <- function(data) {
  data |>
   ggplot2::ggplot(
      ggplot2::aes(x=Inflow, y=Alston)) +
    geom_point(alpha=0.1) +
    ggpmisc::stat_poly_line() +
    ggpmisc::stat_poly_eq(ggpmisc::use_label(c("eq", "R2"))) +
    ggplot2::theme_bw() # +
    # ggplot2::facet_wrap('outflow_type', scales='free', ncol=1)
}

flows4 |>
  lag_calibration_plot()
```
Reasonably good correlation. If it were a perfect correlation, there wouldn't be anything to talk about regarding flow fluctuations. Let's see if there is a better correlation when a lag time is applied to the gaged inflows. 

```{r warning=F}
for(i in c(1,32,45)) {
  print(
    flows4 |>
      dplyr::mutate(Inflow = dplyr::lag(Inflow, i)) |>
      lag_calibration_plot() +
      ggplot2::ggtitle(paste0("Lag ", i, " timesteps")) )
}
```

I reviewed all of the plots and equations for lag times from 1 to 45 timesteps. The best R^2 and regression equations come from lags ranging from 31 to 34 timesteps. Thats 7hr45min to 8.5hrs. That is a long time. Is that correct? Is there something wrong with my analysis? I expected it to be less time. Compare this with the average water velocity at the gage locations to see if it makes sense.

It might be preferable to set the Y-intercept of the regression line to zero. Also, the lag time for each inflow gage could be estimated separately. This analysis could be developed further, but it seems to indicate that there is a lag time of around 8 hours from the inflow gages to the outflow gage. If that is correct, then it should be accounted for if we are going to use an estimate of inflow to evaluate fluctuations in the outflow. 

These results indicate that the gaged inflows should be scaled up 15-20% and delayed by 7.75-8.5 hours to better represent "natural" flows at Alston.

# Flow Fluctuations

Alternatively, it is possible to evaluate the flow fluctuations at the Alston gage without referring to a synthetic "naturalized" streamflow dataset. 

Let's consider only the 15 minute data, and look at the flow fluctuations at the different gauges and as a sum of inflows.
```{r}
flux0 <- flows3 |>
  dplyr::arrange(dateTime) |>
  dplyr::mutate(
    Inflow = (Broad + Enoree + Tyger) * 1.202611) |>
  dplyr::mutate(
    dplyr::across(
      c(Broad, Enoree, Tyger, Alston, Inflow), 
      function(x) {x - dplyr::lag(x)},
      .names="{.col}_delta")) |>
  dplyr::filter(time_interval == 900)

flux1 <- flux0 |>
  dplyr::select(
    dateTime, outflow_type, dplyr::everything(),
    -time_interval) |>
  dplyr::rename(Broad_flow=Broad,
                Enoree_flow=Enoree,
                Tyger_flow=Tyger,
                Alston_flow=Alston,
                Inflow_flow=Inflow) |>
  tidyr::pivot_longer(
    cols=3:12,
    names_to = c('gage', 'name'),
    names_sep='_') |>
  tidyr::pivot_wider() |>
  dplyr::mutate(limb = dplyr::if_else(
    delta>0, 'rising', 'falling'),
    delta = abs(delta))
  
```

```{r eval=F}
flux <- flux1 
usethis::use_data(flux, overwrite=T)
```

```{r warning=FALSE}
flux1 |>
  dplyr::filter(outflow_type == 'Gate operation') |>
  ggplot2::ggplot(
    ggplot2::aes(x=(delta/sqmi))) +
  ggplot2::geom_histogram() +
  # ggplot2::scale_y_log10() +
  ggplot2::scale_x_log10(
    breaks=c(0.001, 0.01, 0.1, 1),
    labels=c(0.001, 0.01, 0.1, 1)) +
  ggplot2::facet_wrap('gage') +
  ggplot2::theme_bw()
```


# Evaluation of Alternative Metrics


```{r eval=F}
# Parr Reservoir Levels
parr0 <- readRDS('../data-raw/parr0.rds')
```

Annual summaries

Plot every March spawning season

Plot cumulative fluctuation metrics and compare their performance under different streamflows (i.e. in March).



